[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Processing event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/DMRC%20DP/src/main/scala/guru/learningjournal/spark/examples/StreamTableJoinDemo.scala","languageId":"scala","version":1,"text":"package guru.learningjournal.spark.examples\n\nimport org.apache.log4j.Logger\nimport org.apache.spark.sql.functions.{col, from_json, to_timestamp}\nimport org.apache.spark.sql.streaming.Trigger\nimport org.apache.spark.sql.types.{StringType, StructField, StructType}\nimport org.apache.spark.sql.{DataFrame, SparkSession}\n\nobject StreamTableJoinDemo extends Serializable {\n  System.setProperty(\"hadoop.home.dir\", \"C:\\\\Projects\\\\SteveWinutils\\\\winutils\\\\hadoop-3.0.0\");\n  @transient lazy val logger: Logger = Logger.getLogger(getClass.getName)\n\n  def main(args: Array[String]): Unit = {\n\n    val spark = SparkSession.builder()\n      .master(\"local[3]\")\n      .appName(\"Stream Table Join Demo\")\n      .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n      .config(\"spark.sql.shuffle.partitions\", 2)\n      .config(\"spark.cassandra.connection.host\", \"localhost\")\n      .config(\"spark.cassandra.connection.port\", \"9042\")\n      .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n      .config(\"spark.sql.catalog.lh\", \"com.datastax.spark.connector.datasource.CassandraCatalog\")\n      .getOrCreate()\n\n    val loginSchema = StructType(List(\n      StructField(\"created_time\", StringType),\n      StructField(\"login_id\", StringType)\n    ))\n\n    val kafkaSourceDF = spark\n      .readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n      .option(\"subscribe\", \"logins\")\n      .option(\"startingOffsets\", \"earliest\")\n      .load()\n\n    val valueDF = kafkaSourceDF.select(from_json(col(\"value\").cast(\"string\"), loginSchema).alias(\"value\"))\n    val loginDF = valueDF.select(\"value.*\")\n      .withColumn(\"created_time\", to_timestamp(col(\"created_time\"), \"yyyy-MM-dd HH:mm:ss\"))\n\n    val userDF = spark.read\n      .format(\"org.apache.spark.sql.cassandra\")\n      .option(\"keyspace\", \"spark_db\")\n      .option(\"table\", \"users\")\n      .load()\n\n    val joinExpr = loginDF.col(\"login_id\") === userDF.col(\"login_id\")\n    val joinType = \"inner\"\n\n    val joinedDF = loginDF.join(userDF, joinExpr, joinType)\n      .drop(loginDF.col(\"login_id\"))\n\n    val outputDF = joinedDF.select(col(\"login_id\"), col(\"user_name\"),\n      col(\"created_time\").alias(\"last_login\"))\n\n    val outputQuery = outputDF.writeStream\n      .foreachBatch(writeToCassandra _)\n      .outputMode(\"update\")\n      .option(\"checkpointLocation\", \"chk-point-dir\")\n      .trigger(Trigger.ProcessingTime(\"1 minute\"))\n      .start()\n    logger.info(\"Waiting for Query\")\n    outputQuery.awaitTermination()\n  }\n\n  def writeToCassandra(outputDF: DataFrame, batchID: Long): Unit = {\n    outputDF.write\n      .format(\"org.apache.spark.sql.cassandra\")\n      .option(\"keyspace\", \"spark_db\")\n      .option(\"table\", \"users\")\n      .mode(\"append\")\n      .save()\n    outputDF.show()\n  }\n}\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0manalysis location (C:\Projects\DMRC DP\target\scala-2.12\zinc\inc_compile_2.12.zip,true)[0m
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 4 s, completed 31 Mar, 2023 6:57:54 PM[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Done event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled request received: shutdown: JsonRpcRequestMessage(2.0, â™¨1, shutdown, null})[0m
